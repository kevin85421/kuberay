apiVersion: v1
kind: Service
metadata:
  name: service-ray-model-training-worker
  labels:
    app: ray-model-training-worker
spec:
  ports:
  - name: gcs-server
    protocol: TCP
    port: 6380
    targetPort: 6380
  selector:
    app: ray-model-training-worker
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-ray-model-training-worker
  labels:
    app: ray-model-training-worker
spec:
  # Change this to scale the number of worker nodes started in the Ray cluster.
  replicas: 2
  selector:
    matchLabels:
      app: ray-model-training-worker
  template:
    metadata:
      labels:
        app: ray-model-training-worker
    spec:
      restartPolicy: Always
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      - name: mesh-manager-tls
        secret:
          secretName: mesh-manager-tls
      containers:
      - name: ray-model-training-worker
        image: rayproject/ray:2.3.0
        imagePullPolicy: Always
        command: ["/bin/bash", "-c", "--"]
        args:
          - "ray start --num-cpus=$MY_CPU_REQUEST --address=service-ray-cluster:6380 --object-manager-port=8076 --node-manager-port=8077 --dashboard-agent-grpc-port=8078 --dashboard-agent-listen-port=52365 --block" 
        ports:
          - containerPort: 6380 # GCS server
        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if it's not a shared memory volume.
        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /etc/mesh-manager/tls
            name: mesh-manager-tls
            readOnly: true
        env:
          - name: RAY_USE_TLS
            value: "1"
          - name: RAY_TLS_SERVER_CERT
            value: "/etc/mesh-manager/tls/tls.crt"
          - name: RAY_TLS_SERVER_KEY
            value: "/etc/mesh-manager/tls/tls.key"
          - name: RAY_TLS_CA_CERT
            value: "/etc/mesh-manager/tls/ca.crt"
          - name: RAY_BACKEND_LOG_LEVEL
            value: warning
          # This is used in the ray start command so that Ray can spawn the
          # correct number of processes. Omitting this may lead to degraded
          # performance.
          - name: MY_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                resource: requests.cpu
          # The resource requests and limits in this config are too small for production!
          # It is better to use a few large Ray pods than many small ones.
          # For production, it is ideal to size each Ray pod to take up the
          # entire Kubernetes node on which it is scheduled.
        resources:
          limits:
            cpu: "1"
            memory: "1G"
            # For production use-cases, we recommend specifying integer CPU reqests and limits.
            # We also recommend setting requests equal to limits for both CPU and memory.
            # For this example, we use a 500m CPU request to accomodate resource-constrained local
            # Kubernetes testing environments such as KinD and minikube.
          requests:
            cpu: "500m"
            memory: "1G"